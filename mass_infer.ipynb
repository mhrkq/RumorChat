{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import html\n",
    "INPUT_FILE = \"mass_infer_data_test.csv\"\n",
    "OUTPUT_FILE = \"mass_infer_responses.csv\"\n",
    "CHATBOT_HOST = \"127.0.0.1:6000\"\n",
    "CHATBOT_URI = f\"http://{CHATBOT_HOST}/v1/chat/completions\"\n",
    "OUTPUT_FOLDER = \"data\"  # Default, should NOT be changed\n",
    "\n",
    "output_path = os.path.join(OUTPUT_FOLDER, OUTPUT_FILE)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "dummy_request = {\n",
    "    # \"user_input\": \"Say yes.\",\n",
    "    \"max_new_tokens\": 10,\n",
    "    \"auto_max_new_tokens\": False,\n",
    "    \"max_tokens_second\": 0,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"say yes\"}],\n",
    "    \"mode\": \"chat-instruct\",  # Valid options: 'chat', 'chat-instruct', 'instruct'\n",
    "    # \"character\": \"Example\",\n",
    "    # \"instruction_template\": \"Vicuna-v1.1\",  # Will get autodetected if unset\n",
    "    # \"your_name\": \"You\",\n",
    "    # # 'name1': 'name of user', # Optional\n",
    "    # # 'name2': 'name of character', # Optional\n",
    "    # # 'context': 'character context', # Optional\n",
    "    # # 'greeting': 'greeting', # Optional\n",
    "    # # 'name1_instruct': 'You', # Optional\n",
    "    # # 'name2_instruct': 'Assistant', # Optional\n",
    "    # # 'context_instruct': 'context_instruct', # Optional\n",
    "    # # 'turn_template': 'turn_template', # Optional\n",
    "    # \"regenerate\": False,\n",
    "    # \"_continue\": False,\n",
    "    # \"chat_instruct_command\": 'Continue the chat dialogue below. Write a single reply for the character \"<|character|>\".\\n\\n<|prompt|>',\n",
    "    # # Generation params. If 'preset' is set to different than 'None', the values\n",
    "    # # in presets/preset-name.yaml are used instead of the individual numbers.\n",
    "    # \"preset\": \"None\",\n",
    "    # \"do_sample\": True,\n",
    "    # \"temperature\": 0.7,\n",
    "    # \"top_p\": 0.1,\n",
    "    # \"typical_p\": 1,\n",
    "    # \"epsilon_cutoff\": 0,  # In units of 1e-4\n",
    "    # \"eta_cutoff\": 0,  # In units of 1e-4\n",
    "    # \"tfs\": 1,\n",
    "    # \"top_a\": 0,\n",
    "    # \"repetition_penalty\": 1.18,\n",
    "    # \"presence_penalty\": 0,\n",
    "    # \"frequency_penalty\": 0,\n",
    "    # \"repetition_penalty_range\": 0,\n",
    "    # \"top_k\": 40,\n",
    "    # \"min_length\": 0,\n",
    "    # \"no_repeat_ngram_size\": 0,\n",
    "    # \"num_beams\": 1,\n",
    "    # \"penalty_alpha\": 0,\n",
    "    # \"length_penalty\": 1,\n",
    "    # \"early_stopping\": False,\n",
    "    # \"mirostat_mode\": 0,\n",
    "    # \"mirostat_tau\": 5,\n",
    "    # \"mirostat_eta\": 0.1,\n",
    "    # \"grammar_string\": \"\",\n",
    "    # \"guidance_scale\": 1,\n",
    "    # \"negative_prompt\": \"\",\n",
    "    # \"seed\": -1,\n",
    "    # \"add_bos_token\": True,\n",
    "    # \"truncation_length\": 2048,\n",
    "    # \"ban_eos_token\": False,\n",
    "    # \"custom_token_bans\": \"\",\n",
    "    # \"skip_special_tokens\": True,\n",
    "    # \"stopping_strings\": [],\n",
    "}\n",
    "\n",
    "\n",
    "# Function to read the input file based on its extension\n",
    "def read_input_file(file_path):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    if file_extension.lower() == \".csv\":\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_extension.lower() in [\".xls\", \".xlsx\"]:\n",
    "        return pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please provide a .csv or .xlsx file.\")\n",
    "\n",
    "\n",
    "# Function to check server connectivity\n",
    "def check_server_connectivity(uri):\n",
    "    try:\n",
    "        response = requests.post(CHATBOT_URI, json=dummy_request)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error checking server connectivity: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Check if the LLM server can be connected to\n",
    "if not check_server_connectivity(CHATBOT_URI):\n",
    "    print(\"Cannot connect to the LLM server. Please check the server status.\")\n",
    "else:\n",
    "    print(\"LLM server is running. Starting inference process...\")\n",
    "    try:\n",
    "        df = read_input_file(INPUT_FILE)\n",
    "        # Replace the string literals with actual newline characters\n",
    "        df.replace({r\"\\\\n\": \"\\n\"}, regex=True, inplace=True)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    # Prepare a DataFrame to store responses\n",
    "    responses = []\n",
    "\n",
    "    # history = {\"internal\": [], \"visible\": []}\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        request_data = {\n",
    "            # \"user_input\": row[\"user_input\"],\n",
    "            \"max_new_tokens\": 500,\n",
    "            \"auto_max_new_tokens\": False,\n",
    "            \"max_tokens_second\": 0,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": row[\"user_input\"]}],\n",
    "            \"mode\": \"chat-instruct\",  # Valid options: 'chat', 'chat-instruct', 'instruct'\n",
    "            # \"character\": \"Example\",\n",
    "            # \"instruction_template\": \"Vicuna-v1.1\",  # Will get autodetected if unset\n",
    "            # \"your_name\": \"You\",\n",
    "            # # 'name1': 'name of user', # Optional\n",
    "            # # 'name2': 'name of character', # Optional\n",
    "            # # 'context': 'character context', # Optional\n",
    "            # # 'greeting': 'greeting', # Optional\n",
    "            # \"name1_instruct\": row[\"name1_instruct\"],\n",
    "            # \"name2_instruct\": row[\"name2_instruct\"],\n",
    "            # \"context_instruct\": row[\"context_instruct\"],\n",
    "            # \"turn_template\": row[\"turn_template\"],\n",
    "            # \"regenerate\": False,\n",
    "            # \"_continue\": False,\n",
    "            # \"chat_instruct_command\": 'Continue the chat dialogue below. Write a single reply for the character \"<|character|>\".\\n\\n<|prompt|>',\n",
    "            # # Generation params. If 'preset' is set to different than 'None', the values\n",
    "            # # in presets/preset-name.yaml are used instead of the individual numbers.\n",
    "            # \"preset\": \"None\",\n",
    "            # \"do_sample\": True,\n",
    "            # \"temperature\": 0.7,\n",
    "            # \"top_p\": 0.1,\n",
    "            # \"typical_p\": 1,\n",
    "            # \"epsilon_cutoff\": 0,  # In units of 1e-4\n",
    "            # \"eta_cutoff\": 0,  # In units of 1e-4\n",
    "            # \"tfs\": 1,\n",
    "            # \"top_a\": 0,\n",
    "            # \"repetition_penalty\": 1.18,\n",
    "            # \"repetition_penalty_range\": 0,\n",
    "            # \"top_k\": 40,\n",
    "            # \"min_length\": 0,\n",
    "            # \"no_repeat_ngram_size\": 0,\n",
    "            # \"num_beams\": 1,\n",
    "            # \"penalty_alpha\": 0,\n",
    "            # \"length_penalty\": 1,\n",
    "            # \"early_stopping\": False,\n",
    "            # \"mirostat_mode\": 0,\n",
    "            # \"mirostat_tau\": 5,\n",
    "            # \"mirostat_eta\": 0.1,\n",
    "            # \"grammar_string\": \"\",\n",
    "            # \"guidance_scale\": 1,\n",
    "            # \"negative_prompt\": \"\",\n",
    "            # \"seed\": -1,\n",
    "            # \"add_bos_token\": True,\n",
    "            # \"truncation_length\": 2048,\n",
    "            # \"ban_eos_token\": False,\n",
    "            # \"custom_token_bans\": \"\",\n",
    "            # \"skip_special_tokens\": True,\n",
    "            # \"stopping_strings\": [],\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Send request to the server\n",
    "            response = requests.post(CHATBOT_URI, json=request_data)\n",
    "\n",
    "            # Process the response\n",
    "            if response.status_code == 200:\n",
    "                # results = response.json()[\"results\"]\n",
    "                chatbot_reply = response.json()['choices'][0]['message']['content']\n",
    "                # Decode HTML entities in the response\n",
    "                chatbot_reply = html.unescape(chatbot_reply)\n",
    "            else:\n",
    "                print(f\"Error with status code: {response.status_code}\")\n",
    "                chatbot_reply = \"Error occurred during request.\"\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            chatbot_reply = \"Server error occurred.\"\n",
    "        print(f\"Row {index+1} response: {chatbot_reply}\")\n",
    "        responses.append(\n",
    "            {\"user_input\": row[\"user_input\"], \"chatbot_reply\": chatbot_reply}\n",
    "        )\n",
    "\n",
    "    # Convert the list of dicts to a DataFrame\n",
    "    responses_df = pd.DataFrame(responses)\n",
    "\n",
    "    # Combine the input df with responses_df to include both user_input and chatbot_reply\n",
    "    final_df = pd.concat([df.iloc[:, :-1], responses_df], axis=1)\n",
    "\n",
    "    # Save the final DataFrame to a new CSV file\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(\"Inference process completed. Responses saved to 'mass_infer_responses.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name1_instruct name2_instruct  \\\n",
      "0          USER:     ASSISTANT:   \n",
      "1          USER:     ASSISTANT:   \n",
      "2          USER:     ASSISTANT:   \n",
      "\n",
      "                                    context_instruct  \\\n",
      "0  A chat between a curious user and an artificia...   \n",
      "1  A chat between a curious user and an artificia...   \n",
      "2  A chat between a curious user and an artificia...   \n",
      "\n",
      "                                       turn_template  \\\n",
      "0  <|user|> <|user-message|>\\n<|bot|> <|bot-messa...   \n",
      "1  <|user|> <|user-message|>\\n<|bot|> <|bot-messa...   \n",
      "2  <|user|> <|user-message|>\\n<|bot|> <|bot-messa...   \n",
      "\n",
      "                                          user_input  \n",
      "0  Write a short story about a lost kitten findin...  \n",
      "1  Explain the theory of relativity in simple terms.  \n",
      "2  Can artificial intelligence ever be considered...  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
